import torch

print('-------------------------------------------------------------------------')
print('\n\n')


# standard deviation grows inside the residual stream
x = torch.zeros(768)
print(f'x:\n{x}')
print(x.std())


'''
Here we observe how a residual connection affects a tensor. Below is the original tensor:
x:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

And the standard deviation is Zero as we see below:
tensor(0.)
'''

'''
Below we will use a residual connection/skip connection and see how it behave.
We are printing the standard deviation of X, and the idea is that x += n**-0.5 * temp_random
initially won't affect X as much, but as soon as it starts to take shape, it will increasingly
starts to affect X. 
In the context of a network, this means that initially we can have X and we won't have to deal with
the issue of the initial parameters being somewhat garbage. But as we progress and start to tune in
the parameters, they will gradually take over and start contributing more.
x.std(): 0.09712520241737366
x.std(): 0.13790999352931976
x.std(): 0.1668008714914322
x.std(): 0.1929783821105957
x.std(): 0.21816998720169067
x.std(): 0.23717299103736877
x.std(): 0.25815263390541077
x.std(): 0.28246450424194336
x.std(): 0.30116331577301025
x.std(): 0.31576377153396606
x.std(): 0.3317395746707916
x.std(): 0.3490616977214813
x.std(): 0.36150598526000977
x.std(): 0.3734700381755829
x.std(): 0.38827940821647644
x.std(): 0.4058297574520111
x.std(): 0.4157017171382904
x.std(): 0.43717437982559204
x.std(): 0.4465753138065338
x.std(): 0.4581921398639679
x.std(): 0.4694164991378784
x.std(): 0.48086678981781006
x.std(): 0.4942260980606079
x.std(): 0.5014308094978333
x.std(): 0.5150507688522339
x.std(): 0.5256069302558899
x.std(): 0.534247100353241
x.std(): 0.5483149886131287
x.std(): 0.5530726313591003
x.std(): 0.5628318190574646
x.std(): 0.57293701171875
x.std(): 0.585793137550354
x.std(): 0.6020395159721375
x.std(): 0.6102585196495056
x.std(): 0.6246616840362549
x.std(): 0.6290408372879028
x.std(): 0.631821870803833
x.std(): 0.6417953372001648
x.std(): 0.6515089273452759
x.std(): 0.6610417366027832
x.std(): 0.6658819913864136
x.std(): 0.6773250699043274
x.std(): 0.6853189468383789
x.std(): 0.6920494437217712
x.std(): 0.703514039516449
x.std(): 0.7091531753540039
x.std(): 0.7133025527000427
x.std(): 0.7204645872116089
x.std(): 0.7278961539268494
x.std(): 0.7351483702659607
x.std(): 0.7407826781272888
x.std(): 0.7493897080421448
x.std(): 0.7519056797027588
x.std(): 0.7584217190742493
x.std(): 0.7621287703514099
x.std(): 0.7588923573493958
x.std(): 0.7643709182739258
x.std(): 0.7699348330497742
x.std(): 0.780754566192627
x.std(): 0.7823371887207031
x.std(): 0.7841942310333252
x.std(): 0.7949250936508179
x.std(): 0.8042743802070618
x.std(): 0.8151596188545227
x.std(): 0.8221917748451233
x.std(): 0.8286608457565308
x.std(): 0.8323440551757812
x.std(): 0.8414890170097351
x.std(): 0.8454132676124573
x.std(): 0.8472447991371155
x.std(): 0.8521589636802673
x.std(): 0.8561676144599915
x.std(): 0.8583492636680603
x.std(): 0.8640240430831909
x.std(): 0.8650617599487305
x.std(): 0.8716307282447815
x.std(): 0.8770262598991394
x.std(): 0.8821841478347778
x.std(): 0.8881121277809143
x.std(): 0.8901847004890442
x.std(): 0.8985943794250488
x.std(): 0.908705472946167
x.std(): 0.9100849628448486
x.std(): 0.9198806285858154
x.std(): 0.9226829409599304
x.std(): 0.9305669665336609
x.std(): 0.9352443814277649
x.std(): 0.9386985301971436
x.std(): 0.9454166889190674
x.std(): 0.9468216300010681
x.std(): 0.947586715221405
x.std(): 0.9488478302955627
x.std(): 0.947735607624054
x.std(): 0.9522230625152588
x.std(): 0.9550734758377075
x.std(): 0.9706119298934937
x.std(): 0.9720308780670166
x.std(): 0.9703888893127441
x.std(): 0.9748931527137756
x.std(): 0.9809423089027405
'''
#------------
n = 100 # e.g. 100 layers
for i in range(n):
    temp_random = torch.randn(768) #100 random numbers

    x += n**-0.5 * temp_random

    print(f'x.std(): {x.std()}')
#------------

# print(f'x:\n{x}')

